{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from module import Preprocessor, Predictor\n",
    "\n",
    "FORMAT = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "logger = logging.getLogger('global_logger')\n",
    "\n",
    "with open('./config/config.yaml', 'r') as config_file:\n",
    "    try:\n",
    "        config = yaml.safe_load(config_file)\n",
    "        preprocessor = Preprocessor(config['preprocessing'], logger)\n",
    "        data_x, data_y, train_x, train_y, validate_x, validate_y, test_x = preprocessor.process()\n",
    "    except yaml.YAMLError as err:\n",
    "        logger.warning('Config file err: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayer(object):\n",
    "    def __init__(self, classes):\n",
    "        self.models = {}\n",
    "        self.classes = classes\n",
    "        for cls in self.classes:\n",
    "            model = MultinomialNB()\n",
    "            self.models[cls] = model\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            class_labels = train_y[:,idx]\n",
    "            self.models[cls].fit(train_x, class_labels)\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        predictions = np.zeros((test_x.shape[0], len(self.classes)))\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            predictions[:, idx] = self.models[cls].predict(test_x)\n",
    "        return predictions\n",
    "\n",
    "    def predict_prob(self, test_x):\n",
    "        probs = np.zeros((test_x.shape[0], len(self.classes)))\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            probs[:, idx] = self.models[cls].predict_proba(test_x)[:,1]\n",
    "        return probs\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, config, logger, classes):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.classes = classes\n",
    "        self._create_model(classes)\n",
    "\n",
    "    def _create_model(self, classes):\n",
    "        if self.config['model_name'] == 'naivebayse':\n",
    "            self.model = NaiveBayer(classes)\n",
    "        else:\n",
    "            self.logger.warning(\"Model Type:{} is not support yet\".format(self.config['model_name']))\n",
    "\n",
    "    def fit_and_validate(self, train_x, train_y, validate_x, validate_y):\n",
    "        self.model.fit(train_x, train_y)\n",
    "        predictions = self.model.predict(validate_x)\n",
    "        #self.metrics(predictions, validate_y)\n",
    "        return self.model, predictions\n",
    "\n",
    "    def metrics(self, predictions, labels):\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        self.logger.info(\"Validate Accuracy:{}\".format(accuracy))\n",
    "        cls_report = classification_report(labels, predictions)\n",
    "        self.logger.info(\"{}\".format(cls_report))\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.model.fit(train_x, train_y)\n",
    "        return self.model\n",
    "\n",
    "trainer = Trainer(config['training'], logger, config['preprocessing']['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, predictions = trainer.fit_and_validate(train_x, train_y, validate_x, validate_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47872, 6)\n"
     ]
    }
   ],
   "source": [
    "validate_labels = np.delete(validate_y, -1, 1)\n",
    "print(validate_labels.shape)\n",
    "accuracy = accuracy_score(validate_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.60      0.68      4591\n",
      "           1       0.42      0.41      0.42       485\n",
      "           2       0.74      0.59      0.65      2527\n",
      "           3       0.14      0.03      0.05       131\n",
      "           4       0.68      0.53      0.59      2362\n",
      "           5       0.31      0.10      0.15       430\n",
      "\n",
      "   micro avg       0.72      0.55      0.62     10526\n",
      "   macro avg       0.51      0.38      0.43     10526\n",
      "weighted avg       0.71      0.55      0.61     10526\n",
      " samples avg       0.98      0.95      0.93     10526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validate_labels, predictions,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.fit(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class Predictor(object):\n",
    "    def __init__(self, config, logger, model):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        predictions = self.model.predict(test_x)\n",
    "        return predictions\n",
    "\n",
    "    def predict_prob(self, test_x):\n",
    "        prob = self.model.predict_prob(test_x)\n",
    "        return prob\n",
    "\n",
    "    def save_result(self, test_ids, probs):\n",
    "        with open(self.config['output_path'], 'w') as output_csv_file:\n",
    "             header = ['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "             writer = csv.writer(output_csv_file)\n",
    "             writer.writerow(header)\n",
    "             for test_id, prob in zip(test_ids, probs.tolist()):\n",
    "                 writer.writerow([test_id] + prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(config['predict'], logger, model)\n",
    "probs = predictor.predict_prob(test_x)\n",
    "predictor.save_result(preprocessor.test_ids, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "predictor.save_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

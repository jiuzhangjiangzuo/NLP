{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines:159571, normal lines:143346 (89.83%)\n",
      "toxic: 15294 (9.58%)\n",
      "severe_toxic: 1595 (1.00%)\n",
      "obscene: 8449 (5.29%)\n",
      "threat: 478 (0.30%)\n",
      "insult: 7877 (4.94%)\n",
      "identity_hate: 1405 (0.88%)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/train.csv\"\n",
    "csv_file = open(file_path)\n",
    "csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "class_counter = {'toxic': 0, 'severe_toxic': 0, 'obscene': 0, 'threat': 0,\n",
    "                 'insult': 0, 'identity_hate': 0}\n",
    "normal_comment_counter = 0\n",
    "global_counter = 0\n",
    "\n",
    "for line in csv_reader:\n",
    "    is_toxic = False\n",
    "    for key in class_counter.keys():\n",
    "        if line[key] == '1':\n",
    "            class_counter[key] += 1\n",
    "            is_toxic = True\n",
    "    if not is_toxic:\n",
    "        normal_comment_counter += 1\n",
    "    global_counter += 1\n",
    "    \n",
    "print(\"total lines:{}, normal lines:{} ({:.2f}%)\".format(global_counter, normal_comment_counter, normal_comment_counter / global_counter * 100))\n",
    "for key, value in class_counter.items():\n",
    "    print(\"{}: {} ({:.2f}%)\".format(key, value, value / global_counter * 100))\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_classes = {\n",
    "    'toxic': 1,\n",
    "    'severe_toxic': 5, \n",
    "    'obscene': 1,\n",
    "    'threat': 20,\n",
    "    'insult': 1,\n",
    "    'identity_hate': 5    \n",
    "}\n",
    "output_file_path = \"../data/upsample_train.csv\"\n",
    "\n",
    "with open(file_path) as input_csv_file, open(output_file_path, 'w') as output_csv_file:\n",
    "    csv_reader = csv.DictReader(input_csv_file)\n",
    "    csv_writer = csv.DictWriter(output_csv_file, fieldnames=csv_reader.fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    for line in csv_reader:\n",
    "        max_upsample_time = 1\n",
    "        for cls, times in upsample_classes.items():\n",
    "            if line[cls] == '1':\n",
    "                max_upsample_time = max(max_upsample_time, times)\n",
    "        for _ in range(max_upsample_time):\n",
    "            csv_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def create_lm(data):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    # 统计3-grams\n",
    "    for line in data:\n",
    "        for i in range(len(line)-2):\n",
    "            w1 = line[i]\n",
    "            w2 = line[i+1]\n",
    "            w3 = line[i+2]\n",
    "\n",
    "            model[(w1, w2)][w3] += 1\n",
    "\n",
    "    # 把计数转换成概率\n",
    "    for prefix in model:\n",
    "        total_count = float(sum(model[prefix].values()))\n",
    "        for word in model[prefix]:\n",
    "            model[prefix][word] /= total_count\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_a_sentence(model):\n",
    "    text = ['<START>', '<START>']\n",
    "    sentence_finished = False\n",
    "\n",
    "    while not sentence_finished:\n",
    "        # 设定一个随机阈值 (增加文本多样性)\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        for word in model[tuple(text[-2:])].keys():\n",
    "            accumulator += model[tuple(text[-2:])][word]\n",
    "            # 当累加概率超过阈值时，选择该词汇\n",
    "            if accumulator >= r:\n",
    "                if word == '<END>':\n",
    "                    sentence_finished = True\n",
    "                else:\n",
    "                    text.append(word)\n",
    "                break\n",
    "        \n",
    "        if len(text) > 64:\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join([t for t in text[2:] if t])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class_collection = {\n",
    "    'toxic': [],\n",
    "    'severe_toxic': [], \n",
    "    'obscene': [],\n",
    "    'threat': [],\n",
    "    'insult': [],\n",
    "    'identity_hate': []\n",
    "}    \n",
    "\n",
    "generated_example_size = {cls: class_counter[cls] * upsample_classes[cls] - 1 for cls in class_collection.keys()}    \n",
    "\n",
    "cls_data_generator_dict = {}\n",
    "\n",
    "with open(file_path) as input_csv_file:\n",
    "    csv_reader = csv.DictReader(input_csv_file)\\\n",
    "    \n",
    "    for line in csv_reader:\n",
    "        for cls in class_collection.keys():\n",
    "            if line[cls] == '1':\n",
    "                class_collection[cls].append(['<START>', '<START>'] + line['comment_text'].strip().split() + ['<END>'])\n",
    "    \n",
    "    for cls, data in class_collection.items():\n",
    "        model = create_lm(data)\n",
    "        cls_data_generator_dict[cls] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = {\n",
    "    'id': '',\n",
    "    'comment_text': '',\n",
    "}\n",
    "\n",
    "for cls in class_collection.keys():\n",
    "    output_file_path = \"../data/argumented_{}.csv\".format(cls)\n",
    "    with open(output_file_path, 'w') as output_csv_file:\n",
    "        csv_writer = csv.DictWriter(output_csv_file, fieldnames=['id', 'comment_text'])\n",
    "        csv_writer.writeheader()\n",
    "        model = cls_data_generator_dict[cls]\n",
    "        for i in range(generated_example_size[cls]):\n",
    "            sentence = generate_a_sentence(model)\n",
    "            new_example = test_example.copy()\n",
    "            new_example['id'] = '{}_{}'.format(cls, i)\n",
    "            new_example['comment_text'] = sentence\n",
    "            csv_writer.writerow(new_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过原始模型预测其它分类的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list = ['severe_toxic', 'threat', 'identity_hate']\n",
    "output_file_path = \"../data/argumented_train.csv\"\n",
    "\n",
    "empty_example = {\n",
    "    'id': '',\n",
    "    'comment_text': '',\n",
    "    'toxic': 0,\n",
    "    'severe_toxic': 0, \n",
    "    'obscene': 0,\n",
    "    'threat': 0,\n",
    "    'insult': 0,\n",
    "    'identity_hate': 0\n",
    "}\n",
    "\n",
    "with open(output_file_path, 'a') as output_csv_file:\n",
    "    csv_writer = csv.DictWriter(output_csv_file, fieldnames=csv_reader.fieldnames)\n",
    "    for cls in cls_list:\n",
    "        cls_text_file = open(\"../data/argumented_{}.csv\".format(cls))\n",
    "        cls_text_file_csv_reader = csv.DictReader(cls_text_file)\n",
    "        cls_prediction_file = open(\"../data/{}_submission.csv\".format(cls))\n",
    "        cls_pred_csv_reader  = csv.DictReader(cls_prediction_file)\n",
    "        for text_line, pred_line in zip(cls_text_file_csv_reader, cls_pred_csv_reader):\n",
    "            assert text_line['id'] == pred_line['id']\n",
    "            new_example = empty_example.copy()\n",
    "            new_example['id'] = text_line['id']\n",
    "            new_example['comment_text'] = text_line['comment_text']\n",
    "            for label in class_collection.keys():\n",
    "                if float(pred_line[label]) > 0.5:\n",
    "                    new_example[label] = 1\n",
    "                else:\n",
    "                    new_example[label] = 0\n",
    "            new_example[cls] = 1\n",
    "            csv_writer.writerow(new_example)\n",
    "        cls_text_file.close()\n",
    "        cls_prediction_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
